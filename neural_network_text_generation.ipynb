{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7300d613-10e7-4d52-a263-98483d7020ec",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad9a5d-9777-4ae9-9a8a-68ecf02686f7",
   "metadata": {},
   "source": [
    "In this exercise, I will build a neural network text generation model using LSTM (Long Short-Term Memory), a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies in sequential data. LSTMs are particularly well-suited for tasks involving sequences, like text generation, because they can retain information over time, handle varying sequence lengths, and process both short- and long-term context. These characteristics make them ideal for applications such as text generation, speech recognition, and time series prediction.\n",
    "\n",
    "For text generation, the ability of LSTMs to remember previous words and phrases is crucial for generating coherent and meaningful text, which is the primary goal of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded61b7e-d5ef-45ff-a39e-ebd269a61a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to import we will be needing to solve this exercise\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import string\n",
    "from keras.callbacks import EarlyStopping\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4143ec4b-9435-497d-b098-c4776116ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 'Frankenstein' successfully!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.gutenberg.org/files/84/84-0.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open('frankenstein.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text.lower())\n",
    "    print(\"Downloaded 'Frankenstein' successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008ffaec-32c3-47c6-85ff-e1304300b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** start of the project gutenberg ebook 84 ***\n",
      "\n",
      "\n",
      "\n",
      "frankenstein;\n",
      "\n",
      "\n",
      "\n",
      "or, the modern prometheus\n",
      "\n",
      "\n",
      "\n",
      "by mary wollstonecraft (godwin) shelley\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " contents\n",
      "\n",
      "\n",
      "\n",
      " letter 1\n",
      "\n",
      " letter 2\n",
      "\n",
      " letter 3\n",
      "\n",
      " letter 4\n",
      "\n",
      " chapter 1\n",
      "\n",
      " chapter 2\n",
      "\n",
      " chapter 3\n",
      "\n",
      " chapter 4\n",
      "\n",
      " chapter 5\n",
      "\n",
      " chapter 6\n",
      "\n",
      " chapter 7\n",
      "\n",
      " chapter 8\n",
      "\n",
      " chapter 9\n",
      "\n",
      " chapter 10\n",
      "\n",
      " chapter 11\n",
      "\n",
      " chapter 12\n",
      "\n",
      " chapter 13\n",
      "\n",
      " chapter 14\n",
      "\n",
      " chapter 15\n",
      "\n",
      " chapter 16\n",
      "\n",
      " chapter 17\n",
      "\n",
      " chapter 18\n",
      "\n",
      " chapter 19\n",
      "\n",
      " chapter 20\n",
      "\n",
      " chapter 21\n",
      "\n",
      " chapter 22\n",
      "\n",
      " chapter 23\n",
      "\n",
      " chapter 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "letter 1\n",
      "\n",
      "\n",
      "\n",
      "_to mrs. saville, england._\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "st. petersburgh, dec. 11th, 17—.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "you will rejoice to hear that no disaster has accompanied the\n",
      "\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "\n",
      "forebodings. i arrived here yesterday, and my first task is to assure\n",
      "\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "\n",
      "of my undertaking.\n",
      "\n",
      "\n",
      "\n",
      "i am already far north of london, and as i walk in the streets of\n",
      "\n",
      "petersburgh, i feel a cold northern breeze \n"
     ]
    }
   ],
   "source": [
    "with open('frankenstein.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the first 500 characters to check the content\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d7c851-1c43-4063-88c3-30fb8f77d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start of the project gutenberg ebook 84 \n",
      "\n",
      "\n",
      "\n",
      "frankenstein;\n",
      "\n",
      "\n",
      "\n",
      "or, the modern prometheus\n",
      "\n",
      "\n",
      "\n",
      "by mary wollstonecraft godwin shelley\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " contents\n",
      "\n",
      "\n",
      "\n",
      " letter 1\n",
      "\n",
      " letter 2\n",
      "\n",
      " letter 3\n",
      "\n",
      " letter 4\n",
      "\n",
      " chapter 1\n",
      "\n",
      " chapter 2\n",
      "\n",
      " chapter 3\n",
      "\n",
      " chapter 4\n",
      "\n",
      " chapter 5\n",
      "\n",
      " chapter 6\n",
      "\n",
      " chapter 7\n",
      "\n",
      " chapter 8\n",
      "\n",
      " chapter 9\n",
      "\n",
      " chapter 10\n",
      "\n",
      " chapter 11\n",
      "\n",
      " chapter 12\n",
      "\n",
      " chapter 13\n",
      "\n",
      " chapter 14\n",
      "\n",
      " chapter 15\n",
      "\n",
      " chapter 16\n",
      "\n",
      " chapter 17\n",
      "\n",
      " chapter 18\n",
      "\n",
      " chapter 19\n",
      "\n",
      " chapter 20\n",
      "\n",
      " chapter 21\n",
      "\n",
      " chapter 22\n",
      "\n",
      " chapter 23\n",
      "\n",
      " chapter 24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "letter 1\n",
      "\n",
      "\n",
      "\n",
      "to mrs. saville, england.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "st. petersburgh, dec. 11th, 17.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "you will rejoice to hear that no disaster has accompanied the\n",
      "\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "\n",
      "forebodings. i arrived here yesterday, and my first task is to assure\n",
      "\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "\n",
      "of my undertaking.\n",
      "\n",
      "\n",
      "\n",
      "i am already far north of london, and as i walk in the streets of\n",
      "\n",
      "petersburgh, i feel a cold northern breeze play upon m\n"
     ]
    }
   ],
   "source": [
    "# Clean the text: remove unwanted characters, keep only letters and basic punctuation\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, keeping letters, numbers, spaces, and common punctuation\n",
    "    text = re.sub(r\"[^a-z0-9,.';:?!\\s]\", '', text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_text = preprocess_text(text)\n",
    "\n",
    "# Check the cleaned text\n",
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd9c2b-6196-4694-84a7-401298ad84fd",
   "metadata": {},
   "source": [
    "After loading the \"text\" we have to tokenize it. The tokenization of a text can be either using words as tokens or characters as tokens. I believe word-level tokenization is often better suited for text generation tasks where meaning, context, and coherence between words are essential. These are some of the advantages I find decisive in the selection of word tokenization for text generation:\n",
    "\n",
    "1. Captures Meaning: Words carry semantic meaning, while characters do not. Tokenizing at the word level helps the model learn higher-level concepts directly.\n",
    "\n",
    "2. Faster Learning: Word-level tokenization reduces the vocabulary size compared to characters, enabling faster model training and better efficiency.\n",
    "\n",
    "3. Better Generalization: It allows the model to recognize common word patterns (e.g., \"the cat\") rather than learning character sequences from scratch.\n",
    "\n",
    "4. Simpler Models: Word-level tokenization results in simpler architectures, requiring less complex processing than character-level models.\n",
    "\n",
    "5. Natural Language Structure: Text generation aligns more with human language, which is structured around words, not individual characters.\n",
    "\n",
    "6. Handling Rare Words: The model can handle rare or complex words as single tokens, avoiding the need to generate them from individual characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f16bc4-ccb0-4022-8d15-66902970a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire text once (using words as tokens, not characters)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])  # Fit the tokenizer to the text corpus (mapping words to integers)\n",
    "total_words = len(tokenizer.word_index) + 1  # Total number of unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa70f264-b3f1-4178-a830-90987f01671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the entire text into a sequence of integers (mapping words to integer indices)\n",
    "tokenized_text = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bad8239-90db-41bf-a11c-846166618836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 75516 sequences.\n",
      "Shape of X: (75516, 5)\n",
      "Shape of y: (75516,)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 5  # The length of the input sequence (how many previous words to consider for the prediction)\n",
    "\n",
    "# Pre-allocate NumPy arrays for efficiency (storing input sequences and their corresponding target words)\n",
    "num_sequences = len(tokenized_text) - sequence_length\n",
    "X = np.zeros((num_sequences, sequence_length), dtype=np.int32)  # Input sequences (X)\n",
    "y = np.zeros(num_sequences, dtype=np.int32)  # Target words (y)\n",
    "\n",
    "# Generate input sequences (X) and their corresponding targets (y)\n",
    "for i in range(num_sequences):\n",
    "    X[i] = tokenized_text[i:i + sequence_length]  # The sequence of tokens as input\n",
    "    y[i] = tokenized_text[i + sequence_length]  # The next word as the target\n",
    "\n",
    "# Convert y to a numpy array of integer type\n",
    "y = np.array(y, dtype=np.int32)\n",
    "\n",
    "# Print shapes to confirm the data generation is correct\n",
    "print(f\"Generated {len(X)} sequences.\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d651aa-f30b-496c-a229-c9cb511f0ded",
   "metadata": {},
   "source": [
    "Now that we tokenized the text and built X and y, we can start building the LSTM\n",
    "\n",
    "**Model Architecture:**\n",
    "\n",
    "Embedding Layer: This converts the input sequence of integer indices into dense vector representations of size 128.\n",
    "\n",
    "LSTM Layer: This layer has 256 hidden units. It is the core of the RNN, which will learn the dependencies between characters in the input sequence.\n",
    "\n",
    "Dropout Layer: This is used to prevent overfitting by randomly setting 20% of the inputs to 0 during training.\n",
    "\n",
    "Dense Layer: This fully connected layer with softmax activation outputs the probabilities for the next character in the sequence. The number of units is equal to the number of unique characters in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83588a6-e85d-4ef4-945e-cf7489e05cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tokenizer's word index to determine the vocabulary size (based on words, not characters)\n",
    "vocab_size = total_words  # Vocabulary size based on the tokenizer (number of unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825b36f5-7515-40eb-a090-5efde3c7d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential([\n",
    "    # Embedding Layer: Maps each integer token to a dense vector of size 128 (adjustable)\n",
    "    Embedding(input_dim=vocab_size, output_dim=128),  # Convert input integers to dense vectors\n",
    "\n",
    "    # LSTM Layer: 256 hidden units, learning temporal dependencies in the data\n",
    "    LSTM(256, return_sequences=True),\n",
    "    \n",
    "    # LSTM Layer: 256 hidden units, learning temporal dependencies in the data\n",
    "    LSTM(256, return_sequences=False),  # return_sequences=False because it's the final LSTM layer\n",
    "    \n",
    "    # Dropout Layer: Prevents overfitting by randomly setting 20% of the LSTM units to 0 during training\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Dense Output Layer: Predicts the next word with softmax activation (multi-class classification)\n",
    "    Dense(vocab_size, activation='softmax')  # Output layer with softmax activation for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6dd7b97-14ef-481b-b322-ff17c4d592de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with SparseCategoricalCrossentropy loss (integer labels)\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e83acc-8242-4c48-8169-06ee556612ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 95ms/step - accuracy: 0.1931 - loss: 4.3399\n",
      "Epoch 2/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 102ms/step - accuracy: 0.2095 - loss: 4.1354\n",
      "Epoch 3/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 96ms/step - accuracy: 0.2233 - loss: 3.9653\n",
      "Epoch 4/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 89ms/step - accuracy: 0.2445 - loss: 3.7802\n",
      "Epoch 5/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 98ms/step - accuracy: 0.2711 - loss: 3.5897\n",
      "Epoch 6/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 99ms/step - accuracy: 0.2939 - loss: 3.4224\n",
      "Epoch 7/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - accuracy: 0.3235 - loss: 3.2431\n",
      "Epoch 8/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 91ms/step - accuracy: 0.3529 - loss: 3.0710\n",
      "Epoch 9/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 101ms/step - accuracy: 0.3807 - loss: 2.9064\n",
      "Epoch 10/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 101ms/step - accuracy: 0.4081 - loss: 2.7712\n",
      "Epoch 11/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 101ms/step - accuracy: 0.4356 - loss: 2.6114\n",
      "Epoch 12/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 95ms/step - accuracy: 0.4622 - loss: 2.4621\n",
      "Epoch 13/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 101ms/step - accuracy: 0.4879 - loss: 2.3379\n",
      "Epoch 14/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 98ms/step - accuracy: 0.5187 - loss: 2.1976\n",
      "Epoch 15/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - accuracy: 0.5445 - loss: 2.0674\n",
      "Epoch 16/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 104ms/step - accuracy: 0.5669 - loss: 1.9665\n",
      "Epoch 17/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 96ms/step - accuracy: 0.5959 - loss: 1.8314\n",
      "Epoch 18/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 94ms/step - accuracy: 0.6174 - loss: 1.7233\n",
      "Epoch 19/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 103ms/step - accuracy: 0.6375 - loss: 1.6264\n",
      "Epoch 20/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 98ms/step - accuracy: 0.6609 - loss: 1.5265\n",
      "Epoch 21/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 95ms/step - accuracy: 0.6729 - loss: 1.4502\n",
      "Epoch 22/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 96ms/step - accuracy: 0.7003 - loss: 1.3403\n",
      "Epoch 23/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 108ms/step - accuracy: 0.7203 - loss: 1.2485\n",
      "Epoch 24/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 109ms/step - accuracy: 0.7396 - loss: 1.1619\n",
      "Epoch 25/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 110ms/step - accuracy: 0.7554 - loss: 1.0943\n",
      "Epoch 26/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 109ms/step - accuracy: 0.7696 - loss: 1.0255\n",
      "Epoch 27/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 109ms/step - accuracy: 0.7867 - loss: 0.9617\n",
      "Epoch 28/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 107ms/step - accuracy: 0.8019 - loss: 0.8868\n",
      "Epoch 29/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 112ms/step - accuracy: 0.8149 - loss: 0.8390\n",
      "Epoch 30/30\n",
      "\u001b[1m590/590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 113ms/step - accuracy: 0.8255 - loss: 0.7792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d40d11d1c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the data\n",
    "# Early stopping to avoid overfitting\n",
    "# Batch size and epochs must be definied taking into account computational power\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(X, y, batch_size=128, epochs=30, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9c394-30fe-495b-a04d-aef37f20615c",
   "metadata": {},
   "source": [
    "Taking into account your computer, define batch size and epochs. If you desire to improve the model, you can keep training it to improve accuracy. Furthermore, I added early stopping combating overfitting and decreasing training time; I highly recommend it! In my particular case, I trained 10 epochs (to test the code and the training time), and after I trained 30 more. I arrived to a pretty good accuracy score; I could go on but my computational resources are already glitching a lot, and to test the model it would be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59ef01b6-5802-4622-aba1-26147bc0565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "model.save(\"frakenstein_tg_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7006e9a9-c95a-445c-afaa-d336e76efc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load already saved model\n",
    "# model = keras.models.load_model(\"frakenstein_tg_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26851c-ff0e-4a6a-a379-6061a1499338",
   "metadata": {},
   "source": [
    "Now let's test the model! Let's try and generate some text for different temperatures.\n",
    "Btw, in case you are not familiar, lower temperature (e.g., 0.5) produces more predictable and coherent text, while higher temperature (e.g., 1.0 or above) produces more creative but sometimes erratic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bb7d58-246f-4b7e-bab6-e15220c020f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(predictions, temperature=1.0):\n",
    "\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions + 1e-8) / temperature  # Apply temperature scaling\n",
    "    exp_preds = np.exp(predictions)\n",
    "    probabilities = exp_preds / np.sum(exp_preds)           # Re-normalize to get a probability distribution\n",
    "\n",
    "    return np.random.choice(len(probabilities), p=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5450a0bf-80e4-48e3-8f07-100fc9bab85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seed_text, sequence_length, num_words_to_generate=50, temperature=1.0):\n",
    "\n",
    "    # Tokenize the seed text\n",
    "    tokenized_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    \n",
    "    # Ensure the seed is long enough (pad with zeros if needed)\n",
    "    if len(tokenized_seed) < sequence_length:\n",
    "        tokenized_seed = [0] * (sequence_length - len(tokenized_seed)) + tokenized_seed\n",
    "\n",
    "    # Initialize the generated text with the seed text\n",
    "    generated_text = seed_text\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Prepare the input sequence for the model\n",
    "        input_sequence = np.array(tokenized_seed[-sequence_length:]).reshape(1, sequence_length)\n",
    "\n",
    "        # Predict the next word probabilities\n",
    "        predicted_probabilities = model.predict(input_sequence, verbose=0)[0]\n",
    "\n",
    "        # Sample the next word index using temperature scaling\n",
    "        predicted_word_index = sample_with_temperature(predicted_probabilities, temperature=temperature)\n",
    "\n",
    "        # Convert the predicted index back to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Break if the predicted word is an empty string (unknown word)\n",
    "        if predicted_word == '':\n",
    "            break\n",
    "\n",
    "        # Append the predicted word to the generated text\n",
    "        generated_text += ' ' + predicted_word\n",
    "\n",
    "        # Add the predicted word index to the sequence for the next prediction\n",
    "        tokenized_seed.append(predicted_word_index)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27dde49-8772-4831-9f61-0c0d5284a8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Temperature (0.5):\n",
      "I had worked hard for nearly two years, for the sole purpose of infusing life into an inanimate body for this i had deprived him of rest and health he had gone before i learned that they also would\n",
      "\n",
      "Medium Temperature (1.0):\n",
      "I had worked hard for nearly two years, for the sole purpose of infusing life into an inanimate body for this will i have been rather a shattered and pleasant snatched around the mountains which had now the arbiters\n",
      "\n",
      "High Temperature (1.5):\n",
      "I had worked hard for nearly two years, for the sole purpose of infusing life into an inanimate body with mine they had nursed on and a family lest my mind should you have spent unguarded and human will\n"
     ]
    }
   ],
   "source": [
    "# Example seed text\n",
    "seed_text = \"I had worked hard for nearly two years, for the sole purpose of infusing life into an inanimate body\"\n",
    "\n",
    "# Generate text with different temperatures\n",
    "print(\"Low Temperature (0.5):\")\n",
    "print(generate_text(model, tokenizer, seed_text, sequence_length=5, num_words_to_generate=20, temperature=0.5))\n",
    "\n",
    "print(\"\\nMedium Temperature (1.0):\")\n",
    "print(generate_text(model, tokenizer, seed_text, sequence_length=5, num_words_to_generate=20, temperature=1.0))\n",
    "\n",
    "print(\"\\nHigh Temperature (1.5):\")\n",
    "print(generate_text(model, tokenizer, seed_text, sequence_length=5, num_words_to_generate=20, temperature=1.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9898061-561d-4283-b500-4b3abbf9da44",
   "metadata": {},
   "source": [
    "In this notebook, we successfully trained a model to generate text based on *Frankenstein* by Mary Shelley. The results were fascinating, showing how temperature settings influenced the creativity of the generated text. With lower temperatures, the model produced more predictable and coherent results that closely mirrored the original text. As the temperature increased, the generated text became more diverse and creative, introducing interesting variations while maintaining some connection to the source material.\n",
    "\n",
    "Despite the increasing randomness at higher temperatures, the model was still able to produce thought-provoking outputs, showcasing its potential for creative writing and storytelling. These results demonstrate that even with simple models, we can explore text generation in a way that pushes the boundaries of classical works like *Frankenstein*.\n",
    "\n",
    "Looking forward, there are opportunities to fine-tune the model for better coherence and creativity, as well as to explore more advanced techniques like transformers for even more impressive text generation. This experiment opens the door to endless possibilities in generating unique and engaging narratives from classic literature.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
